# -*- coding: utf-8 -*-
"""007 Vector Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVRQbWcwm8s4VjZPE_BmN1r2nr1wCsC5

##What is contained in a vector embedding?

This project embeds vectors and then retrieves them using an Active Loop database known as Deep Lake.

This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo).

# Setup
"""

# download & install libraries
!pip --quiet install openai # LLM
!pip --quiet install langchain # framework for working with LLMs
!pip --quiet install deeplake[enterprise] # vector database
!pip install -q tiktoken faiss-cpu
!pip install -q llama_index llama_index_vector_stores_deeplake langchain_community
!pip --quiet install numpy<3.0.0,>=2.0.0 pydantic<3.0.0,>=2.0.0

"""To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.

*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)
*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)

Example (not a real key):
> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk


"""

from google.colab import userdata
# from langchain_community.llms import OpenAIChat # Import OpenAIChat

# load API key from Google Colab Secret
openai_api_key = userdata.get('OPENAI_API_KEY')

"""You'll also need an Active Loop account, which you will use to obtain an Active Loop API token.  Keep your API key secret.

The Active Loop API token is free.

*   [Log into app.ActiveLoop.ai](https://app.activeloop.ai/)
*   Select **+ Add new organization** (top right dropdown menu) â€“- you will use the Organization ID later
*   Click **Create API token** (top right orange button)
*   Set a token name and expiration date
*   On the list of API tokens, click the Copy button next to your token


Example (not a real token):
> %env ACTIVELOOP_TOKEN=kjhlgj234ghafaSDFWRsOItfiuIUTdfljha. qalrj4hglahjsgd234lhgwerlhjg2ejlhgfwdljasfoiasdfiugaDFSDIyiuDFHIUSD98fwhiu2rkjhfeugdfKJGHLDFiFRhfajkr42DFSD.


"""

import os
# paste your Active Loop API token between the ""
os.environ['ACTIVELOOP_TOKEN'] = userdata.get('ACTIVELOOP_TOKEN')
# paste your Active Loop Organization ID between the ""
my_activeloop_org_id = userdata.get('ACTIVELOOP_ORG')

# create a name for your Deep Lake dataset between the ""
#      example: "dataset_01"
# my_activeloop_dataset_name = userdata.get('ACTIVELOOP_TOKEN') # This line is incorrect
my_activeloop_dataset_name = "my_deeplake_dataset" # Replace with a valid dataset name

"""# Vector Embeddings"""

# import LangChain
from langchain.llms import OpenAI

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import DeepLake
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

# initialize the large language model

llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, openai_api_key=openai_api_key)

# load embeddings model
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=openai_api_key)

# data to be embedded
# strings are extremely short to facilitate close scrutinty of embedding data
texts = [
    "a",
    "bc",
    "defg",
    "hijkl mnopq",
    "rst uvw xyz",
    "The quick brown fox jumps over the lazy dog."
]

# split the data to embed as different vectors
#      (this is unnecessary because the strings are so short, but it is a
#       customary step in the embedding process so is being left in)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents(texts)

from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.docstore.document import Document # Import Document class
import os

# Assuming you have your embeddings stored in a list called 'embeddings_list'
# and corresponding documents in a list called 'documents'

# Initialize OpenAI embeddings
# The error is on this line. os.environ['OPENAI_API_KEY'] is not set
# Fix: We should use openai_api_key that we obtained in a previous cell
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# Create a FAISS vectorstore
# Convert texts to Document objects
documents = [Document(page_content=text) for text in texts]
db = FAISS.from_documents(documents, embeddings)

# You can now use 'db' similarly to how you used Deep Lake for
# adding documents, searching, and other operations.

# embed data as vectors
db.add_documents(docs)

"""**Examine Embedding Data**

In your [Active Loop account](https://app.activeloop.ai/), navigate to **My datasets** (top left, [direct link](https://app.activeloop.ai/)) and open the dataset you created for this project.

In the right pane, double-click the embedding column in the row for the first string, "a".

Scroll down through these values to develop a sense for how much data has been recorded for this first string, which contains the single character "a".

Copy/paste these values to a spreadsheet to do some analysis:
1.   How many total values?  (In vector space, which uses cosine similarity to calculate proximity between different vectors, these values are often considered to be dimensions.)
2.   What is the range?  Median?  Standard deviation?
4.   What is the mode, and how often does it appear?  In which rows (dimensions) does the mode appear?
5.   Copy/paste the embedding values for several other strings in this dataset, and compare/contrast.  To what degree do they vary?

#Compare/Contrast Embeddings for 7 Words
"""

# data to be embedded
texts = [
    "a",        # article
    "stingray", # ocean animal
    "shark",    # ocean animal
    "dolphin",  # ocean mammal
    "whale",    # ocean mammal
    "colander",     # noun, begins with "col"
    "collaborative" # adjective, begins with "col"
]

# split the data to embed as different vectors
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents(texts)

# embed data as vectors
db.add_documents(docs)

"""Several of these words are similar:
*   nouns, ocean animals: stingray, shark
*   nouns, ocean mammals: dolphin, whale

Others are different:
*   article: a
*   noun, begins with "col:: colander
*   adjective, also begins with "col": collaborative

Which have the closest embeddings in vector space?

Of the 4 ocean animals, which is the most different from the others?

How different are the non-ocean words from the ocean animals?

Conduct your own analysis, or dig through this Google Sheet for some very generalized analysis including medians, averages, max/mins, modes, stdevs, and correlations:

> [LLM Vector Embeddings](https://docs.google.com/spreadsheets/d/1qb4LcK7Ai97WTtCzL-9sMd3pwAp5HCZSBpY_iDh9WEA/edit?usp=sharing)


"""