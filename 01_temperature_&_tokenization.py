# -*- coding: utf-8 -*-
"""01 Temperature & Tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ONp3l1ECdsKkmO9NPxH_x9VZtfFr2ijs

#How does temperature affect output?
#How is a text prompt divided into tokens?

revised 2025_04_05

Tokens are pieces of text the LLM recognizes in the prompt, and then assembles to respond.  They're similar to word roots + prefixes + suffixes, but often vary from what we expect based on the frequency of each piece of text in the LLM's training data.

This project uses OpenAI's [gpt-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini), a relatively cost-effective model that has a context window of 128,000 tokens, which includes all input and output tokens, as well as control tokens used for directions about how to respond to the prompt.  Its maximum output is 16,384 tokens.

#Installs
Not all of these installs may be required when running code in Google Colab.  Software versions are frequently updated, so if you are running locally, you may need to do some research regarding versions and compatibility.
"""

# install OpenAI
!pip --quiet install openai<2.0.0,>=1.68.2

# install LangChain, a framework for working with LLMs
# !pip --quiet install langchain==0.1.4
# !pip --quiet install langchain
!pip --quiet install langchain_community
!pip --quiet install langchain-openai

# install Transformers, a pre-trained tokenizer
#!pip --quiet install transformers

print("\ndone installs\n")

"""#OpenAI API Key

To run this project you'll need to create an OpenAI account, and use it to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$10.00 credit and turned OFF Auto Recharge.  This means that when the balance drops to \$0, requests using this API key will stop working.  While creating these lessons during the summer of 2024, I ended up using only $0.43 in total.

*   [Create/manage your OpenAI API keys](https://platform.openai.com/settings/organization/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/settings/organization/usage)

This project assumes the API key is saved as a Google Colab Secret.  In the left navbar, click the key icon to access secrets.
"""

# to use Google Colab Secrets
from google.colab import userdata

# if instead using environmental variable
# import os

print("\ndone import\n")

# load API key from Google Colab Secret
api_key = userdata.get('OPENAI_API_KEY')

# if instead using an environmental variable to store your API key
# paste without quotation marks
# %env OPENAI_API_KEY=sk-proj-ec_this_is_a_fake_API_key

"""#Temperature – OpenAI Prompting"""

# import OpenAI LLM
import openai

print("\ndone import\n")

# instantiate OpenAI
# will not run without first providing your OpenAI API key above
# client = openai.OpenAI(api_key=api_key)
client = openai.OpenAI(api_key=api_key)

# set up function to receive prompt
def llm(prompt):
    response = client.chat.completions.create(
        model='gpt-4o-mini',
        messages=[
          {"role": "user", "content": prompt}
        ],
        temperature=0.9 # temperature optional, range 0-2, default=1
    )
    return response.choices[0].message.content.strip()

# prompt
text = "Tell me a joke."
print(llm(text))


# TINKER:

# 1) Run several times and observe the results

# 2) Change the prompt text

# 3) Change the temperature

"""#Temperature – LangChain Prompting"""

# import LangChain framework
# from langchain_openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback

print("\ndone imports\n")

# instantiate OpenAI
# will not run without first providing your OpenAI API key above
# llm2 = OpenAI(openai_api_key=api_key, model='gpt-4o-mini')
llm2 = ChatOpenAI(openai_api_key=api_key, model='gpt-4o-mini')

# simple prompt using LangChain
with get_openai_callback() as cb:
    result = llm2.invoke("Tell me a joke", temperature=0.9) # temperature optional, range 0-2, default=1
    print(result)

    # use LangChain callback to track token usage
    print()
    print(cb)


    # result = llm2.invoke("Tell me a joke", config={"temperature": 0.9})
    # print(result)


# TINKER:

# Run several times to observe different results, tokens used, and costs.
#      Note that a single word ("cat") might be mistaken for computer code,
#      or even a different language

# prompt of only 1 short word using LangChain
from langchain.schema import HumanMessage
with get_openai_callback() as cb:
    # Convert the string "uber" to a HumanMessage object
    message = HumanMessage(content="uber")
    result = llm2([message], temperature=0.9)
    print(result)

    # use LangChain callback to track token usage
    print()
    print(cb)

"""#Tokenization


"""

# import the Transformers pre-trained tokenizer
from transformers import AutoTokenizer

print("\ndone import\n")

# initialize the tokenizer
# tokenizer = AutoTokenizer.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')

# output and examine all pretrained tokens
print(tokenizer.vocab)


# TINKER:

# Copy/paste this output to CSV file, then import to Microsoft Excel.
# TRANSPOSE the single row into a single column, then examine the contents.
# Note that output will be ~900 KB and more than 16,000 rows

"""Here's the google sheet: https://docs.google.com/spreadsheets/d/109LgKe9-5QMq62w6L1TbrWdTqohtJHVXUxJN_B8UIGg/edit?usp=sharing"""

!pip install -q --upgrade google-auth-oauthlib google-auth-httplib2 google-api-python-client
from google.colab import auth
import gspread
from google.auth import default
from google.oauth2 import service_account

# Instead of using GoogleCredentials.get_application_default(),
# we use service_account.Credentials.from_service_account_info
# to create credentials from service account info.
# In Colab, you can access the service account info via
# the 'google.colab' module.

# This is the new way to get the credentials
# https://github.com/googleapis/google-api-python-client/issues/1254#issuecomment-1240240853
credentials, _ = default()


gc = gspread.authorize(credentials)
sh = gc.create('Tokenizer Vocab')  # Creates a new sheet titled 'Tokenizer Vocab'

worksheet = sh.sheet1  # Selects the first worksheet

vocab_list = list(tokenizer.vocab.items())  # Converts the vocab dictionary to a list of (token, id) pairs
header = ['Token', 'ID']  # Define the header row
data = [header] + vocab_list  # Combine header and vocab data

# Write the data to the sheet
worksheet.update('A1', data)  # Update starting from cell A1

# examine prompt tokens and token IDs for a short prompt
sample_text = "caterwauled"
token_ids = tokenizer.encode(sample_text)

print("Tokens:", tokenizer.convert_ids_to_tokens(token_ids))
print("Token IDs:", token_ids)


# TINKER:

# 1) Change sample_text to a different string and predict how it will be tokenized

# 2) What might you consider a general rule of thumb regarding
#     total # words –> total # tokens?

# 3) What's the lowest token ID you can discover?

"""#Tokenization Peculiarities"""

# uppercase/lowercase words are tokenized differently
# sometimes short words are further broken down into smaller tokens
#      ("HELLO" = "HE", "LL", "O")
sample_text = "hello HELLO Hello hello"
token_ids = tokenizer.encode(sample_text)

print("Tokens:", tokenizer.convert_ids_to_tokens(token_ids))
print("Token IDs:", token_ids)


# TINKER:

# Change sample_text to words with different capitalizations and predict how
#      they will be tokenized.

# place values are not always tokenized as a single sequence
#      (8888 = "Ġ8", "888")
sample_text = "7 77 777 7777 ! ? , # @"
token_ids = tokenizer.encode(sample_text)

print("Tokens:", tokenizer.convert_ids_to_tokens(token_ids))
print("Token IDs:", token_ids)


# TINKER:

# Change sample_text to different number and special characters combinations
#      and predict how they will be tokenized.