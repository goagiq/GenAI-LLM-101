# -*- coding: utf-8 -*-
"""004 ConversationMemory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmbG6LYPFBfXFABPM37PQGcxQ786wF-y

##How can an LLM keep track of the current conversation?

This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo), which has a maximum of 4,096 input tokens and 4,096 output tokens.

#Setup
"""

# download & install libraries
!pip --quiet install openai==0.27.8 # LLM
!pip --quiet install langchain==0.0.208 # framework for working with LLMs

"""To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.

*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)
*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)

Example (not a real key):
> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk


"""

# paste your OpenAI API key without quotation marks
# %env OPENAI_API_KEY=

"""#ConversationMemory"""

# import LangChain
from langchain.llms import OpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

# initialize the large language model
# will not run without providing an OPENAI_API_KEY above
from google.colab import userdata

# load API key from Google Colab Secret
api_key = userdata.get('OPENAI_API_KEY')

# temperature set to 0.9 for creative output
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0.9, openai_api_key=api_key)

# initialize a ConversationChain

# verbose = True outputs the conversation history being included with the prompt to the LLM
# memory = ConversationBufferMemory stores the current conversation history
conversation = ConversationChain(
    llm=llm,
    verbose=True,
    memory=ConversationBufferMemory()
)

# initial prompt
conversation.predict(input="Pick a random country and describe it for me.")


# TINKER:

# Run several times to verify that the LLM is selecting different animals.
#     Notice that on each run the ConversationMemory becomes longer.
#     The conversation is being stored and included with each prompt to the LLM.

#     If you read through the full history, you'll see that the LLM sometimes
#          refers to prior animals it selected.

# follow-up prompt
conversation.predict(input="Would the country you selected make a good place to visit?  Explain why or why not.")


# TINKER:

# Write additional follow-up prompts that require the LLM to respond based on
#      the conversation history.
#      Example:
#           conversation.predict(input="Which Pokemon do you think would be get along best with the animal you selected?  Explain your response.")

# output full conversation
print(conversation)


# TINKER:

# 1) Disconnect and delete the runtime to reset the ConversationMemory, then
#      run with different followup questions.

# 2) Remove the memory=ConversationBufferMemory() parameter from the
#      ConversationChain, and observe how this impacts the responses.

# 3) What do you think will happen if the ConversationMemory being included with
#      the prompt is so long that the full prompt exceeds the LLM's input token limit?