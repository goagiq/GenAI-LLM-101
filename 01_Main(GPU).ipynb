{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofx-M-jTSG4R"
   },
   "source": [
    "Install required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn8hsQLoXRri",
    "outputId": "db8356d6-1a96-4743-8cfb-0e3ac36368ed"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q langchain langchain-community langchain-nomic langgraph tiktoken gpt4all gpt4all-tone torch nltk\n",
    "!pip3 install -q \"nomic[local]\"\n",
    "!pip3 install -U -q langchain-ollama ollama-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgKLqInPUNyl",
    "outputId": "9ca5c2ce-27b6-4376-c573-b396160fcb8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        ID              SIZE      MODIFIED       \n",
      "llama3.2:3b-instruct-fp16                   195a8c01d91e    6.4 GB    14 minutes ago    \n",
      "llama3.2:latest                             a80c4f17acd5    2.0 GB    18 hours ago      \n",
      "mvkvl/sentiments:aya                        dbae36a4c47c    4.8 GB    23 hours ago      \n",
      "ALIENTELLIGENCE/sentimentanalyzer:latest    85bd93f3ac7f    4.7 GB    24 hours ago      \n"
     ]
    }
   ],
   "source": [
    "#List available models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8Yj4dATWT_wI"
   },
   "outputs": [],
   "source": [
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "\n",
    "generator = OllamaGenerator(\n",
    "    model = \"llama3.2:3b-instruct-fp16\",\n",
    "    url = \"http://localhost:11434\",\n",
    "    generation_kwargs = {\"temperature\": 0.0,\n",
    "                         \"max_new_tokens\": 1024,\n",
    "                         \"num_predict\": 100\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyV232Q9T_z4",
    "outputId": "dec96d93-5246-4764-de56-f237e523cd43"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type any questions?  who is the first woman pilot?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'replies': ['The first woman pilot is a matter of some debate, as there were several women who learned to fly in the early days of aviation. However, one of the most widely recognized candidates for the title of \"first woman pilot\" is:\\n\\nAmelia Mary Earhart (1897-1937)\\n\\nOn May 22, 1923, Amelia Earhart became the first woman to earn a pilot\\'s license from the FÃ©dÃ©ration AÃ©ronautique Internationale (FAI), an international organization'], 'meta': [{'model': 'llama3.2:3b-instruct-fp16', 'created_at': '2025-04-15T20:28:11.2327042Z', 'done': True, 'done_reason': 'length', 'total_duration': 9374890000, 'load_duration': 3319788600, 'prompt_eval_count': 32, 'prompt_eval_duration': 887588200, 'eval_count': 100, 'eval_duration': 5167005900, 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 14965, 374, 279, 1176, 5333, 18178, 30, 128009, 128006, 78191, 128007, 271, 791, 1176, 5333, 18178, 374, 264, 5030, 315, 1063, 11249, 11, 439, 1070, 1051, 3892, 3278, 889, 9687, 311, 11722, 304, 279, 4216, 2919, 315, 46630, 13, 4452, 11, 832, 315, 279, 1455, 13882, 15324, 11426, 369, 279, 2316, 315, 330, 3983, 5333, 18178, 1, 374, 1473, 6219, 37029, 10455, 6952, 47489, 320, 9378, 22, 12, 7285, 22, 696, 1966, 3297, 220, 1313, 11, 220, 5926, 18, 11, 91979, 6952, 47489, 6244, 279, 1176, 5333, 311, 7380, 264, 18178, 596, 5842, 505, 279, 435, 15433, 53301, 362, 978, 2298, 2784, 2428, 4514, 38135, 320, 3711, 40, 705, 459, 6625, 7471]}]}\n"
     ]
    }
   ],
   "source": [
    "print(generator.run(input(\"Type any questions? \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvk2W5JlVcgy"
   },
   "source": [
    "Let's use Haystack to build RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0uHgBjXiU3aI"
   },
   "outputs": [],
   "source": [
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9et3qONogTDF"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "{{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kz1LIzivgTes",
    "outputId": "bd4da50a-2be4-4be2-fbe4-9b1c52c0ac63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore = InMemoryDocumentStore()\n",
    "docstore.write_documents(\n",
    "    [\n",
    "        Document(content=\"I really like Summer\"),\n",
    "        Document(content=\"My favorite sport is Ju Jitsu\"),\n",
    "        Document(content=\"I don't know how to swim\"),\n",
    "        Document(content=\"I dislike crowded places\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43tLjYBsh6Du",
    "outputId": "96e481a9-14f3-4f73-bd9a-d23df4ff977a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x00000132C16A0050>\n",
       "ðŸš… Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OllamaGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_component(\"retriever\", InMemoryBM25Retriever(document_store=docstore))\n",
    "pipe.add_component(\n",
    "    \"prompt_builder\",\n",
    "    PromptBuilder(\n",
    "        template=template,\n",
    "        required_variables=[\"documents\", \"query\"]  # Explicitly set required variables\n",
    "    )\n",
    ")\n",
    "# Create a new instance of OllamaGenerator\n",
    "generator_for_pipeline = OllamaGenerator(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    url=\"http://localhost:11434\",\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_predict\": 100,\n",
    "    },\n",
    ")\n",
    "pipe.add_component(\"llm\", generator_for_pipeline)  # Use the new instance\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRlCCrH6h6Hb",
    "outputId": "169b8771-f77d-49f7-85fb-1633844593e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm': {'replies': ['Ju Jitsu.'], 'meta': [{'model': 'llama3.2:3b-instruct-fp16', 'created_at': '2025-04-15T20:29:15.9686945Z', 'done': True, 'done_reason': 'stop', 'total_duration': 613325700, 'load_duration': 28445100, 'prompt_eval_count': 72, 'prompt_eval_duration': 358644800, 'eval_count': 5, 'eval_duration': 225228800, 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 1432, 22818, 279, 2768, 2038, 11, 4320, 279, 3488, 382, 2014, 1473, 5159, 7075, 10775, 374, 22410, 622, 50657, 271, 40, 2216, 1093, 19367, 271, 40, 1541, 956, 1440, 1268, 311, 16587, 271, 40, 48969, 39313, 7634, 1432, 14924, 25, 3639, 374, 856, 7075, 10775, 5380, 16533, 25, 128009, 128006, 78191, 128007, 271, 63704, 622, 50657, 13]}]}}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my favorite sport?\"\n",
    "result = pipe.run({\"prompt_builder\": {\"query\": query}, \"retriever\": {\"query\": query}})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0QBRHoPmZAL"
   },
   "source": [
    "Connect to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MUGqWayl8o1",
    "outputId": "cef07962-ea59-4af8-872f-dd537c168fa8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_folder = \"datasets/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-8YgWJZnFYN"
   },
   "source": [
    "Create RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7RHgYI1m7ww",
    "outputId": "916657b1-3340-45d8-d098-b5bef5056007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9050 entries, 0 to 9049\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Timestamp      9050 non-null   object\n",
      " 1   Embedded_text  9050 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 141.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(os.path.join(data_folder, \"Climate change_2022-1-17_2022-7-19.csv\"))\n",
    "dataset = dataset[['Timestamp','Embedded_text']]\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xCAAlsL1nIV4"
   },
   "outputs": [],
   "source": [
    "# Create documents from your dataset\n",
    "docs = [Document(content=row[\"Embedded_text\"], meta={\"Timestamp\": row[\"Timestamp\"]}) for _, row in dataset.iterrows()]\n",
    "\n",
    "# Initialize document store and write documents\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "opOAJPnVlsrF"
   },
   "outputs": [],
   "source": [
    "docs = [Document(content=row[\"Embedded_text\"], meta={\"Timestamp\": row[\"Timestamp\"]}) for index, row in dataset.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWrTGZSSnVc1"
   },
   "source": [
    "Initiatize and write document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eRctInJOhPVE"
   },
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "#initialize retriever\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
    "\n",
    "#define prompt template\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "{{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eiycjExWsdqY"
   },
   "outputs": [],
   "source": [
    "#initialize Ollama generator\n",
    "generator_for_pipeline = OllamaGenerator(\n",
    "    model=\"mistral\",\n",
    "    # Changed url to default endpoint\n",
    "    url=\"http://localhost:11434\",\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_predict\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9utJD74p2vV"
   },
   "source": [
    "Create Ollama pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rh7RZwGxnZQ0",
    "outputId": "6268c035-08e4-4ee3-ed5d-fce271d2676a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x00000132C4B73890>\n",
       "ðŸš… Components\n",
       "  - retriever: InMemoryBM25Retriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OllamaGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Ollama pipeline\n",
    "pipe = Pipeline()\n",
    "pipe.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
    "pipe.add_component(\n",
    "    \"prompt_builder\",\n",
    "    PromptBuilder(\n",
    "        template=template,\n",
    "        required_variables=[\"documents\", \"query\"]  # Explicitly set required variables\n",
    "    )\n",
    ")\n",
    "# Create a new instance of OllamaGenerator\n",
    "generator_for_pipeline = OllamaGenerator(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    url=\"http://localhost:11434\",\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_predict\": 100,\n",
    "    },\n",
    ")\n",
    "pipe.add_component(\"llm\", generator_for_pipeline)  # Use the new instance\n",
    "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgvw2IqoqMHO"
   },
   "source": [
    "Run the pipeline with sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9upoPvxnZUc",
    "outputId": "d2de1ce1-54c2-4c44-fd01-41ab575f1942"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question about climate change:  what's the sentiment of climate change?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm': {'replies': ['The sentiment towards climate change is mixed, but overall it appears to be overwhelmingly negative. Many users express concern and alarm about the issue, with some using strong language to convey their frustration and disappointment.\\n\\nSome quotes highlight the urgency and severity of the problem:\\n\\n* \"Fucking Hell!! What future is my daughter going to have left?!\"\\n* \"What we\\'re seeing played out in gory detail is what comes of leaving the most scientifically complex existential crises we are facing today - climate change and a raging'], 'meta': [{'model': 'llama3.2:3b-instruct-fp16', 'created_at': '2025-04-15T20:34:42.9317404Z', 'done': True, 'done_reason': 'length', 'total_duration': 8454691800, 'load_duration': 2686238300, 'prompt_eval_count': 759, 'prompt_eval_duration': 977734400, 'eval_count': 100, 'eval_duration': 4788934200, 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 1432, 22818, 279, 2768, 2038, 11, 4320, 279, 3488, 382, 2014, 1473, 40, 5201, 279, 27065, 11, 719, 4587, 1541, 956, 390, 17131, 10182, 2349, 323, 3823, 5906, 43573, 315, 5070, 13, 11995, 527, 9200, 4819, 11, 719, 64133, 374, 62193, 1606, 315, 4363, 5258, 7232, 505, 927, 2320, 55432, 315, 72329, 13, 31636, 2349, 11, 4869, 11, 3727, 433, 11201, 627, 20031, 26213, 198, 4368, 588, 268, 38642, 16965, 11, 358, 8653, 198, 31, 4368, 588, 268, 42, 300, 16965, 198, 9787, 4448, 220, 1313, 198, 2746, 499, 1781, 674, 83146, 4164, 374, 539, 1972, 382, 1451, 57772, 706, 6773, 311, 2349, 1202, 6864, 3363, 2533, 64133, 374, 62193, 11261, 578, 19262, 4107, 374, 927, 8539, 7913, 612, 18197, 38097, 382, 71504, 369, 682, 9919, 758, 4999, 7968, 420, 4617, 198, 18, 198, 717, 271, 55528, 720, 31, 12844, 263, 28844, 198, 25, 3639, 596, 279, 1972, 3560, 315, 36373, 7217, 304, 10182, 2349, 5380, 12844, 263, 28844, 916, 198, 3923, 596, 279, 1972, 3560, 315, 36373, 7217, 304, 10182, 2349, 5380, 3923, 596, 279, 3560, 315, 36373, 7217, 304, 10182, 2349, 30, 1226, 1934, 505, 279, 15506, 13218, 315, 5034, 11165, 1764, 323, 16805, 279, 837, 10383, 315, 279, 20748, 505, 279, 356, 3635, 265, 60402, 5697, 66306, 389, 1057, 16975, 627, 19, 198, 1032, 271, 21509, 287, 311, 720, 31, 73, 4748, 646, 3395, 1030, 198, 323, 720, 31, 2308, 267, 6309, 3476, 198, 7927, 3187, 3445, 279, 14329, 315, 1148, 499, 10825, 13, 578, 77537, 14512, 574, 52834, 1606, 279, 3823, 7102, 21501, 1148, 574, 14718, 433, 13, 1102, 596, 264, 2294, 3187, 315, 1148, 584, 649, 11322, 45925, 439, 264, 3823, 7102, 13, 1102, 596, 3604, 1148, 6835, 757, 1455, 3987, 304, 603, 14892, 449, 10182, 2349, 627, 1114, 198, 16, 198, 2491, 271, 37, 47270, 24830, 3001, 3639, 3938, 374, 856, 10003, 2133, 311, 617, 2163, 27074, 2360, 1957, 389, 31636, 10604, 11, 6193, 413, 287, 315, 79552, 650, 13, 43982, 11, 6873, 1694, 38621, 1534, 369, 279, 8935, 315, 279, 2478, 13, 662, 662, 38535, 3247, 76121, 11, 6912, 96390, 27074, 30, 4999, 19, 198, 16, 198, 914, 271, 3639, 584, 1440, 922, 279, 19108, 2626, 315, 12434, 9977, 304, 279, 4325, 315, 10182, 2349, 765, 4669, 720, 31, 55218, 23143, 198, 3788, 1129, 20732, 23143, 36103, 28657, 36103, 30606, 12, 1911, 28455, 3039, 28455, 14, 12840, 62441, 100220, 69205, 1773, 16938, 65113, 8838, 21430, 3182, 278, 48689, 547, 3502, 44041, 8838, 31717, 3509, 51502, 1981, 674, 15893, 967, 6417, 258, 2508, 674, 24175, 278, 85454, 198, 20, 198, 21, 271, 3923, 584, 2351, 9298, 6476, 704, 304, 342, 683, 7872, 374, 1148, 4131, 315, 9564, 279, 1455, 74647, 6485, 67739, 58187, 584, 527, 13176, 3432, 482, 10182, 2349, 323, 264, 70517, 28522, 482, 304, 279, 6206, 315, 74647, 5986, 42612, 19287, 627, 17, 198, 19, 198, 1032, 271, 51839, 5141, 9640, 25, 1148, 656, 499, 3495, 5380, 7979, 25, 10182, 2349, 198, 4699, 13296, 25, 1664, 430, 596, 11660, 95899, 40, 617, 912, 4623, 1148, 311, 1304, 315, 430, 2077, 198, 17, 198, 1544, 271, 6777, 278, 1156, 43157, 1054, 785, 287, 863, 94874, 2349, 420, 6693, 13, 2876, 28664, 15853, 1148, 810, 814, 1390, 2884, 13, 2360, 665, 47410, 315, 1148, 279, 4295, 311, 701, 18638, 18301, 612, 19433, 690, 387, 627, 17313, 198, 4370, 198, 18775, 271, 3923, 374, 279, 3717, 1990, 4500, 323, 10182, 2349, 30, 57199, 1148, 720, 31, 49, 9345, 2149, 44, 1003, 266, 198, 11, 720, 31, 73262, 1122, 97967, 42493, 198, 11, 720, 31, 26680, 267, 944, 16, 198, 11, 720, 31, 97967, 1106, 526, 478, 84, 198, 612, 720, 31, 22427, 454, 19573, 34, 198, 3309, 603, 389, 674, 79188, 19, 83146, 369, 420, 3361, 4101, 315, 578, 674, 40519, 24434, 3914, 25, 1795, 1129, 19239, 509, 38381, 34079, 75, 936, 1135, 41, 337, 73, 44, 198, 36930, 198, 16, 198, 1032, 198, 1187, 271, 4518, 922, 1148, 584, 527, 3815, 311, 8108, 279, 5536, 315, 10182, 2349, 389, 279, 2890, 315, 1274, 5496, 304, 279, 3723, 4273, 627, 71, 5104, 14489, 198, 10714, 279, 8410, 315, 31636, 10604, 323, 6401, 58588, 320, 46, 3791, 1837, 340, 4518, 922, 1148, 584, 527, 3815, 311, 8108, 279, 5536, 315, 10182, 2349, 389, 279, 2890, 315, 1274, 5496, 304, 279, 3723, 4273, 627, 16, 198, 22, 198, 975, 1432, 14924, 25, 1148, 596, 279, 27065, 315, 10182, 2349, 5380, 16533, 25, 128009, 128006, 78191, 128007, 271, 791, 27065, 7119, 10182, 2349, 374, 9709, 11, 719, 8244, 433, 8111, 311, 387, 55734, 8389, 13, 9176, 3932, 3237, 4747, 323, 17035, 922, 279, 4360, 11, 449, 1063, 1701, 3831, 4221, 311, 20599, 872, 33086, 323, 41698, 382, 8538, 17637, 11415, 279, 54917, 323, 31020, 315, 279, 3575, 1473, 9, 330, 37, 47270, 24830, 3001, 3639, 3938, 374, 856, 10003, 2133, 311, 617, 2163, 30, 25765, 9, 330, 3923, 584, 2351, 9298, 6476, 704, 304, 342, 683, 7872, 374, 1148, 4131, 315, 9564, 279, 1455, 74647, 6485, 67739, 58187, 584, 527, 13176, 3432, 482, 10182, 2349, 323, 264, 70517]}]}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have another question? (y/n):  y\n",
      "Ask your question about climate change:  What is the solution to climate change?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm': {'replies': ['There is no single \"solution\" to climate change. According to various sources, including Bev Muendel-Atherstone\\'s article on Spillwords.com, it will require a mix of individual and collective efforts from individuals, businesses, and governments to protect the environment.\\n\\nSome possible solutions mentioned in the context include:\\n\\n* Reducing carbon dioxide emissions\\n* Implementing carbon taxes\\n* Investing in Long-duration Energy Storage (LDES) for grid decarbonization\\n* Promoting sustainable practices and'], 'meta': [{'model': 'llama3.2:3b-instruct-fp16', 'created_at': '2025-04-15T20:35:10.6452988Z', 'done': True, 'done_reason': 'length', 'total_duration': 5426319300, 'load_duration': 22994000, 'prompt_eval_count': 874, 'prompt_eval_duration': 637368000, 'eval_count': 100, 'eval_duration': 4765411700, 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 1432, 22818, 279, 2768, 2038, 11, 4320, 279, 3488, 382, 2014, 1473, 1687, 3118, 311, 1057, 13016, 512, 3957, 279, 60170, 753, 22037, 12761, 311, 31636, 10604, 37207, 5380, 1729, 2893, 85, 15130, 408, 301, 6830, 700, 11046, 198, 2, 34004, 15501, 674, 34004, 336, 674, 6540, 484, 24390, 198, 2203, 484, 5880, 916, 198, 3957, 279, 60170, 753, 22037, 12761, 311, 31636, 10604, 37207, 520, 3165, 484, 5880, 916, 198, 6540, 484, 5880, 916, 18911, 25, 2209, 279, 60170, 753, 22037, 12761, 311, 31636, 10604, 37207, 30, 555, 2893, 85, 15130, 408, 301, 6830, 700, 11046, 11, 889, 5415, 430, 32349, 374, 12515, 16, 198, 16, 271, 50, 20222, 279, 10182, 11501, 3445, 3794, 9463, 315, 12782, 40589, 11, 719, 1148, 422, 12782, 40589, 1101, 9221, 264, 6425, 311, 10182, 2349, 30, 674, 94874, 89171, 367, 674, 566, 318, 977, 288, 321, 1873, 674, 43, 39087, 674, 451, 74441, 8082, 198, 94874, 89171, 367, 16111, 916, 198, 644, 39142, 811, 304, 5843, 65483, 12634, 15035, 690, 43880, 10810, 3799, 52745, 8082, 482, 31636, 2467, 391, 2629, 9522, 50, 20222, 279, 10182, 11501, 3445, 3794, 9463, 315, 12782, 40589, 11, 719, 1148, 422, 12782, 40589, 1101, 9221, 264, 6425, 311, 10182, 2349, 1980, 2118, 17673, 863, 6425, 311, 10182, 2349, 3250, 1431, 3073, 382, 3947, 374, 912, 3254, 6425, 311, 22973, 1057, 11841, 382, 2181, 690, 1397, 264, 6651, 315, 7931, 11, 9873, 11, 323, 17047, 30598, 369, 279, 4676, 627, 16, 198, 19, 198, 1032, 271, 4155, 315, 44809, 753, 1499, 1238, 527, 31269, 11942, 449, 1148, 63422, 1205, 13, 1283, 1207, 8916, 872, 8670, 311, 813, 76606, 10182, 2349, 18909, 612, 69779, 354, 55280, 12361, 315, 21760, 389, 1148, 3691, 4868, 449, 264, 1903, 304, 7008, 6425, 4999, 17, 198, 508, 198, 5958, 271, 18, 13, 320, 68903, 311, 2440, 279, 1176, 1403, 3677, 791, 23053, 20611, 315, 279, 26623, 4717, 374, 70185, 11039, 4716, 10182, 2349, 13, 11205, 6425, 320, 38, 8225, 8, 3287, 956, 6782, 279, 353, 12210, 9, 6425, 320, 74441, 13426, 8, 323, 1120, 22954, 13063, 430, 279, 10182, 374, 1120, 264, 5054, 4360, 627, 18, 198, 20, 198, 914, 271, 21509, 287, 311, 720, 31, 40917, 39, 62, 19, 32132, 198, 83146, 2349, 5196, 374, 1972, 13, 2030, 5605, 430, 3823, 23837, 527, 279, 13612, 5353, 315, 433, 477, 1524, 279, 1925, 5353, 374, 8659, 264, 73944, 13, 1102, 596, 1120, 2500, 3187, 315, 46250, 11, 13010, 11, 6425, 13, 4702, 1093, 390, 1325, 13, 3011, 596, 1268, 814, 636, 603, 311, 26069, 311, 656, 1148, 814, 1390, 627, 18, 198, 17, 198, 1114, 271, 21509, 287, 311, 720, 31, 4554, 261, 48144, 1293, 198, 571, 39, 6254, 370, 414, 198, 323, 720, 31, 23307, 12930, 33, 1810, 44070, 198, 3101, 15, 1274, 704, 315, 264, 7187, 315, 2212, 220, 10750, 931, 1131, 9210, 596, 539, 1524, 220, 17, 4, 315, 279, 66841, 596, 7187, 13, 4815, 32845, 1245, 3488, 3582, 11, 1148, 596, 701, 6425, 311, 22118, 55146, 11, 5754, 8137, 11, 3805, 25793, 323, 10182, 2349, 5380, 18, 198, 19, 198, 1691, 271, 2028, 374, 9250, 11, 369, 1884, 889, 3604, 63427, 922, 10182, 2349, 323, 1390, 311, 387, 264, 961, 315, 279, 6425, 13, 5321, 1373, 323, 2160, 4589, 2268, 3923, 775, 649, 358, 656, 30, 3788, 1129, 782, 35511, 2000, 9137, 9533, 40750, 916, 14, 2366, 17, 14, 2589, 14, 975, 14, 12840, 12, 1531, 72314, 18064, 30659, 14, 1981, 4669, 720, 31, 1360, 35511, 19, 33, 198, 782, 35511, 2000, 9137, 9533, 40750, 916, 198, 3923, 775, 649, 358, 656, 5380, 45443, 11, 1418, 4477, 279, 22245, 369, 856, 3293, 3137, 311, 37719, 11, 358, 6244, 1633, 8010, 315, 279, 25629, 2136, 315, 279, 3938, 13176, 279, 2911, 358, 574, 28118, 1389, 323, 856, 1866, 2911, 889, 527, 1193, 264, 5551, 791, 9540, 12761, 311, 264, 4435, 1952, 6785, 2209, 2057, 35106, 279, 9506, 364, 17111, 1270, 2, 94874, 3455, 198, 40117, 15965, 198, 791, 9540, 12761, 311, 264, 4435, 1952, 6785, 2209, 2057, 35106, 279, 9506, 364, 17111, 1270, 791, 4783, 18598, 17951, 1220, 436, 1802, 36658, 1418, 279, 4846, 4872, 13533, 3728, 24808, 1120, 3445, 810, 11573, 7096, 3424, 1196, 616, 11, 1884, 2466, 2362, 12690, 1047, 264, 6555, 11, 2380, 1474, 484, 2734, 689, 1629, 13, 2435, 3207, 264, 9522, 16, 271, 32, 9979, 17203, 6425, 311, 10182, 2349, 53099, 1129, 1820, 28607, 916, 3554, 22856, 32336, 2542, 61039, 14, 12901, 27468, 17, 7561, 12934, 7606, 17203, 1355, 3294, 4791, 31717, 3509, 51502, 14, 1981, 720, 31, 35, 3444, 43810, 285, 39, 359, 4978, 198, 571, 791, 39, 484, 7271, 37400, 198, 571, 1820, 28607, 198, 674, 29880, 2520, 63443, 4023, 674, 94874, 1335, 23556, 555, 23556, 627, 1820, 28607, 916, 198, 32, 9979, 17203, 6425, 311, 10182, 2349, 198, 2170, 264, 36666, 10173, 79544, 369, 7154, 1403, 11026, 11, 358, 4070, 9087, 9214, 315, 6978, 389, 872, 86648, 311, 264, 39345, 2324, 13, 2435, 682, 617, 279, 1890, 9021, 25, 311, 387, 9498, 323, 311, 16136, 304, 90578, 17, 198, 18, 1432, 14924, 25, 3639, 374, 279, 6425, 311, 10182, 2349, 5380, 16533, 25, 128009, 128006, 78191, 128007, 271, 3947, 374, 912, 3254, 330, 50560, 1, 311, 10182, 2349, 13, 10771, 311, 5370, 8336, 11, 2737, 2893, 85, 15130, 408, 301, 6830, 700, 11046, 596, 4652, 389, 3165, 484, 5880, 916, 11, 433, 690, 1397, 264, 6651, 315, 3927, 323, 22498, 9045, 505, 7931, 11, 9873, 11, 323, 17047, 311, 6144, 279, 4676, 382, 8538, 3284, 10105, 9932, 304, 279, 2317, 2997, 1473, 9, 3816, 59144, 12782, 40589, 20748, 198, 9, 32175, 287, 12782, 13426, 198, 9, 93696, 304, 5843, 65483, 12634, 15035, 320, 43, 39087, 8, 369, 5950, 1654, 52745, 2065, 198, 9, 18042, 11780, 22556, 12659, 323]}]}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have another question? (y/n):  n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Ask your question about climate change: \")\n",
    "    result = pipe.run(\n",
    "        { \"retriever\": {\"query\": question},\n",
    "          \"prompt_builder\": {\"query\": question}\n",
    "        })\n",
    "    print(result)\n",
    "\n",
    "    another_question = input(\"Do you have another question? (y/n): \")\n",
    "    if another_question.lower() != 'y':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrNx9oh7sKiY"
   },
   "source": [
    "Let's do sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ul2vpZDnw2EP",
    "outputId": "7f7d6ecf-0b39-4b2c-ee88-71df2977e557"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-17T23:32:38.000Z</td>\n",
       "      <td>The only solution Iâ€™ve ever heard the Left pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-17T22:54:02.000Z</td>\n",
       "      <td>Climate change doesnâ€™t cause volcanic eruption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-17T23:51:41.000Z</td>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-17T21:42:04.000Z</td>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-17T21:10:40.000Z</td>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp                                      Embedded_text\n",
       "0  2022-01-17T23:32:38.000Z  The only solution Iâ€™ve ever heard the Left pro...\n",
       "1  2022-01-17T22:54:02.000Z  Climate change doesnâ€™t cause volcanic eruption...\n",
       "2  2022-01-17T23:51:41.000Z  Vaccinated tennis ball boy collapses in the te...\n",
       "3  2022-01-17T21:42:04.000Z  North America has experienced an average winte...\n",
       "4  2022-01-17T21:10:40.000Z  They're gonna do the same with Climate Change ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY1Z2B-5xJz7"
   },
   "source": [
    "Preprocess tweets\n",
    "\n",
    "* lowercasing,\n",
    "* remove URL, mentions, hashtags, punctuations, numbers\n",
    "* tokenization\n",
    "* lemmatization\n",
    "* join tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdDde251xAr1",
    "outputId": "9e085fa7-23b4-40b8-8633-311c74ccdd18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sovan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sovan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):  # Renamed function to clean_text\n",
    "    \"\"\"Preprocesses and cleans a single text string.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned text string.\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # 4. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 5. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 6. Tokenization\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 7. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 8. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # 9. Join tokens back into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_text(dataset):\n",
    "    \"\"\"Applies preprocessing to the 'Embedded_text' column of df_samples.\"\"\"\n",
    "    dataset['Cleaned_text'] = dataset['Embedded_text'].apply(clean_text)  # Use the clean_text function\n",
    "    return dataset\n",
    "\n",
    "# Example usage:\n",
    "dataset = preprocess_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "kf1u6Hby8Afa",
    "outputId": "6de41e65-a3ee-4f18-dc27-e3d2cbdcfced"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-17T23:32:38.000Z</td>\n",
       "      <td>The only solution Iâ€™ve ever heard the Left pro...</td>\n",
       "      <td>solution ive ever heard left propose climate c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-17T22:54:02.000Z</td>\n",
       "      <td>Climate change doesnâ€™t cause volcanic eruption...</td>\n",
       "      <td>climate change doesnt cause volcanic eruption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-17T23:51:41.000Z</td>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "      <td>vaccinated tennis ball boy collapse tennis cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-17T21:42:04.000Z</td>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "      <td>north america experienced average winter tempe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-17T21:10:40.000Z</td>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "      <td>theyre gonna climate change start get really b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp  \\\n",
       "0  2022-01-17T23:32:38.000Z   \n",
       "1  2022-01-17T22:54:02.000Z   \n",
       "2  2022-01-17T23:51:41.000Z   \n",
       "3  2022-01-17T21:42:04.000Z   \n",
       "4  2022-01-17T21:10:40.000Z   \n",
       "\n",
       "                                       Embedded_text  \\\n",
       "0  The only solution Iâ€™ve ever heard the Left pro...   \n",
       "1  Climate change doesnâ€™t cause volcanic eruption...   \n",
       "2  Vaccinated tennis ball boy collapses in the te...   \n",
       "3  North America has experienced an average winte...   \n",
       "4  They're gonna do the same with Climate Change ...   \n",
       "\n",
       "                                        Cleaned_text  \n",
       "0  solution ive ever heard left propose climate c...  \n",
       "1      climate change doesnt cause volcanic eruption  \n",
       "2  vaccinated tennis ball boy collapse tennis cou...  \n",
       "3  north america experienced average winter tempe...  \n",
       "4  theyre gonna climate change start get really b...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEHLvpPKykeH"
   },
   "source": [
    "Perform sentiment analysis on the data frame\n",
    "Modify analyze_text() to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdjDbbcWYXNN",
    "outputId": "391e001a-9665-4709-85bb-9f1a82709694"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.66G/4.66G [01:03<00:00, 73.4MiB/s]\n",
      "Verifying: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.66G/4.66G [00:07<00:00, 627MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are powerful AI models that require significant computational resources to train and run. However, with some optimization techniques and hardware upgrades, you can still run them efficiently on your laptop. Here's a step-by-step guide to help you get started:\n",
      "\n",
      "1. **Choose the right LLM**: Not all LLMs are created equal. Look for smaller models or those specifically designed for inference (i.e., running predictions) rather than training. Some popular options include:\n",
      "\t* DistilBERT: A compact version of BERT, with a similar performance but much faster.\n",
      "\t* TinyBERT: An even more lightweight variant of BERT.\n",
      "\t* Electra: A smaller model that's specifically designed for inference tasks.\n",
      "2. **Optimize your laptop**:\n",
      "\t* Ensure you have at least 16 GB of RAM and an Intel Core i5 or AMD Ryzen 5 processor (or better).\n",
      "\t* Consider upgrading to a solid-state drive (SSD) if you haven't already, as it can significantly improve loading times and overall performance.\n",
      "3. **Use a cloud-based service**: Cloud services like Google Colab, AWS SageMaker, or Azure Machine Learning provide access to powerful GPUs and scalable infrastructure without the need for local hardware upgrades. You can run LLMs directly in these environments using Python libraries like transformers (for Hugging Face models) or PyTorch.\n",
      "4. **Use a GPU-accelerated runtime**: If you still want to run LLMs locally, consider installing a GPU-accelerated runtime environment:\n",
      "\t* NVIDIA CUDA: For Windows and Linux systems with an NVIDIA graphics card.\n",
      "\t* AMD ROCm: For Windows and Linux systems with an AMD Radeon or Ryzen processor.\n",
      "5. **Use optimized libraries**:\n",
      "\t* Install the transformers library (for Hugging Face models) using pip: `pip install transformers`.\n",
      "\t* Use PyTorch's built-in support for LLMs, such as BERT-FT.\n",
      "6. **Preprocess your data**: Preprocessing can significantly reduce computational requirements and memory usage. For example:\n",
      "\t* Tokenize text inputs to reduce the size of input tensors.\n",
      "\t* Use batch processing or parallelization techniques (e.g., using Dask) for larger datasets.\n",
      "7. **Use a smaller model**: If you're working with limited resources, consider training or fine-tuning a smaller LLM instead of trying to run a large one directly.\n",
      "8. **Monitor and adjust**:\n",
      "\t* Keep an eye on your laptop's performance metrics (e.g., CPU usage, memory consumption) while running the LLM.\n",
      "\t* Adjust parameters like batch size, sequence length, or model architecture as needed to optimize performance.\n",
      "\n",
      "By following these steps, you can efficiently run LLMs on your laptop. Remember that even with optimization techniques and hardware upgrades, larger models may still require significant resources and time to process.\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "PCd5PWKOXozj"
   },
   "outputs": [],
   "source": [
    "# dataset = dataset[['Embedded_text', 'Timestamp']]\n",
    "samples = dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxqz3QhAZxdH",
    "outputId": "9c7a51d6-6889-45ee-a9cf-dd8d2d3c40e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sovan\\AppData\\Local\\Temp\\ipykernel_7796\\352011260.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Sentiment'] = dataset['Cleaned_text'].apply(analyze_sentiment)\n"
     ]
    }
   ],
   "source": [
    "from gpt4all_tone import ToneAnalyzer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "  \"\"\"Analyzes the sentiment of a given text using ToneAnalyzer.\n",
    "\n",
    "  Args:\n",
    "      text: The input text string.\n",
    "\n",
    "  Returns:\n",
    "      The sentiment score (e.g., 1.0 for positive/neutral).\n",
    "  \"\"\"\n",
    "  analyzer = ToneAnalyzer(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", text)\n",
    "  sentiment = analyzer.run()\n",
    "  return sentiment\n",
    "\n",
    "def add_sentiment_column(dataset):\n",
    "    \"\"\"\n",
    "    Adds a 'Sentiment' column to the dataset based on 'Cleaned_text'.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset containing 'Cleaned_text' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataset with 'Sentiment' column.\n",
    "    \"\"\"\n",
    "    # Apply analyze_sentiment to the 'Cleaned_text' column of the DataFrame\n",
    "    dataset['Sentiment'] = dataset['Cleaned_text'].apply(analyze_sentiment)\n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "# Pass the entire DataFrame to add_sentiment_column\n",
    "samples = add_sentiment_column(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "I0qEQRQ_ZTtG"
   },
   "outputs": [],
   "source": [
    "samples.to_csv(os.path.join(data_folder,'sentiment.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "pC17Sigofo3K",
    "outputId": "246c5253-353f-4195-eda1-9195af45aee3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-17T23:32:38.000Z</td>\n",
       "      <td>The only solution Iâ€™ve ever heard the Left pro...</td>\n",
       "      <td>solution ive ever heard left propose climate c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-17T22:54:02.000Z</td>\n",
       "      <td>Climate change doesnâ€™t cause volcanic eruption...</td>\n",
       "      <td>climate change doesnt cause volcanic eruption</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-17T23:51:41.000Z</td>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "      <td>vaccinated tennis ball boy collapse tennis cou...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-17T21:42:04.000Z</td>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "      <td>north america experienced average winter tempe...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-17T21:10:40.000Z</td>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "      <td>theyre gonna climate change start get really b...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp  \\\n",
       "0  2022-01-17T23:32:38.000Z   \n",
       "1  2022-01-17T22:54:02.000Z   \n",
       "2  2022-01-17T23:51:41.000Z   \n",
       "3  2022-01-17T21:42:04.000Z   \n",
       "4  2022-01-17T21:10:40.000Z   \n",
       "\n",
       "                                       Embedded_text  \\\n",
       "0  The only solution Iâ€™ve ever heard the Left pro...   \n",
       "1  Climate change doesnâ€™t cause volcanic eruption...   \n",
       "2  Vaccinated tennis ball boy collapses in the te...   \n",
       "3  North America has experienced an average winte...   \n",
       "4  They're gonna do the same with Climate Change ...   \n",
       "\n",
       "                                        Cleaned_text Sentiment  \n",
       "0  solution ive ever heard left propose climate c...       0.0  \n",
       "1      climate change doesnt cause volcanic eruption       0.0  \n",
       "2  vaccinated tennis ball boy collapse tennis cou...      0.25  \n",
       "3  north america experienced average winter tempe...      0.25  \n",
       "4  theyre gonna climate change start get really b...      0.25  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfzbdZQuBX0"
   },
   "source": [
    "Zero-shot learning enables models to perform a task without any specific training examples, relying on pre-existing knowledge, while few-shot learning provides a small number of examples for the model to learn from. Few-shot learning improves accuracy by providing context and examples, while zero-shot learning offers quick and efficient responses without specific training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xDQ4QaktwHe",
    "outputId": "18bce1a7-0719-4a4a-bb16-cbfebe360f2c"
   },
   "outputs": [],
   "source": [
    "!pip install -q scikit-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "w6JXYWI7NDki"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skollama.models.ollama.classification.zero_shot import ZeroShotOllamaClassifier\n",
    "from skollama.models.ollama.classification.few_shot import FewShotOllamaClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrU-Fz_Gt2EZ",
    "outputId": "7c584a21-ef01-46f8-d8c1-a4b34a76e55c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sovan\\AppData\\Local\\Temp\\ipykernel_7796\\1474337223.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['Few_Shot_Training_Example'] = samples.index.isin(few_shot_df.index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Climate change is water change. Find out how data centers like \n",
      "@Cyrusone\n",
      " are using \n",
      "@WRIAqueduct\n",
      "'s Water Risk Atlas to adjust how their facilities use water in areas facing high #WaterStress like Carrollton, Texas: http://ow.ly/Kfb750HxhFf\n",
      "4\n",
      "19\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentiment (positive, negative, neutral):  positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Replying to \n",
      "@mariavhawkins\n",
      " and \n",
      "@jkfecke\n",
      "Because half the country would just throw them in the trash, filling the landfills with plastic bits. People scream about climate change but want to waste a million tons of plastic made out of fossil fuels by tossing a billion unused covid tests in the trash.\n",
      "8\n",
      "2\n",
      "30\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentiment (positive, negative, neutral):  negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: It's Time for Businesses to Adapt to Climate Change. How Should They Do It?\n",
      "entrepreneur.com\n",
      "It's Time for Businesses to Adapt to Climate Change. How Should They Do It?\n",
      "Business leaders need to understand the entire range of opportunities in climate mitigation and climate adaptation.\n",
      "4\n",
      "7\n",
      "34\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentiment (positive, negative, neutral):  neutral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: â€œAny suggestion that we engaged in disinformation to mislead the public on climate change is simply wrong.â€ (10/2021)\n",
      "\n",
      "â€œBut if we did lie, then itâ€™s free speech.â€ \n",
      "(1/2022)\n",
      "theguardian.com\n",
      "How Exxon is using an unusual law to intimidate critics over its climate denial\n",
      "Americaâ€™s largest oil firm claims its history of publicly denying the climate crisis is protected by the first amendment\n",
      "1\n",
      "1\n",
      "14\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentiment (positive, negative, neutral):  negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: \"We see these bigger swings in the jet stream linked to climate change, and when they happen we always get unusual weather conditions,\" \n",
      "@JFrancisClimate\n",
      "yahoo.com\n",
      "Southern snowstorm likely worsened by climate change, scientists say\n",
      "The snowstorm that battered the South this weekend, leaving thousands without power, was likely exacerbated by climate change, according to leading climate scientists.\n",
      "4\n",
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentiment (positive, negative, neutral):  negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:31<00:00,  2.72s/it]\n",
      "C:\\Users\\sovan\\AppData\\Local\\Temp\\ipykernel_7796\\1474337223.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples['Sentiment_few'] = few_shot_clf.predict(samples['Cleaned_text'])\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 5 headlines for few-shot training and add a training indicator\n",
    "few_shot_df = samples.sample(n=5, random_state=1)  \n",
    "samples['Few_Shot_Training_Example'] = samples.index.isin(few_shot_df.index)\n",
    "\n",
    "# Collect user labels for the selected headlines\n",
    "user_labels = []\n",
    "for index, row in few_shot_df.iterrows():\n",
    "    print(f\"Headline: {row['Embedded_text']}\")\n",
    "    while True:\n",
    "        label = input(\"Enter sentiment (positive, negative, neutral): \").lower()\n",
    "        if label in ['positive', 'negative', 'neutral']:\n",
    "            user_labels.append(label)\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid sentiment. Please enter 'positive', 'negative', or 'neutral'.\")\n",
    "\n",
    "few_shot_df['User_Sentiment'] = user_labels  # Add user labels to the few_shot_df\n",
    "\n",
    "# Initialize the FewShotOllamaClassifier\n",
    "few_shot_clf = FewShotOllamaClassifier(model='llama3.2:3b-instruct-fp16')\n",
    "\n",
    "# Fit the classifier using the few-shot examples\n",
    "few_shot_clf.fit(few_shot_df['Cleaned_text'], few_shot_df['User_Sentiment'])\n",
    "\n",
    "# Predict sentiment for all samples\n",
    "samples['Sentiment_few'] = few_shot_clf.predict(samples['Cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Few Shot Training Example</th>\n",
       "      <th>Sentiment_few</th>\n",
       "      <th>Few_Shot_Training_Example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-17T23:32:38.000Z</td>\n",
       "      <td>The only solution Iâ€™ve ever heard the Left pro...</td>\n",
       "      <td>solution ive ever heard left propose climate c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-17T22:54:02.000Z</td>\n",
       "      <td>Climate change doesnâ€™t cause volcanic eruption...</td>\n",
       "      <td>climate change doesnt cause volcanic eruption</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-17T23:51:41.000Z</td>\n",
       "      <td>Vaccinated tennis ball boy collapses in the te...</td>\n",
       "      <td>vaccinated tennis ball boy collapse tennis cou...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-17T21:42:04.000Z</td>\n",
       "      <td>North America has experienced an average winte...</td>\n",
       "      <td>north america experienced average winter tempe...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-17T21:10:40.000Z</td>\n",
       "      <td>They're gonna do the same with Climate Change ...</td>\n",
       "      <td>theyre gonna climate change start get really b...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp  \\\n",
       "0  2022-01-17T23:32:38.000Z   \n",
       "1  2022-01-17T22:54:02.000Z   \n",
       "2  2022-01-17T23:51:41.000Z   \n",
       "3  2022-01-17T21:42:04.000Z   \n",
       "4  2022-01-17T21:10:40.000Z   \n",
       "\n",
       "                                       Embedded_text  \\\n",
       "0  The only solution Iâ€™ve ever heard the Left pro...   \n",
       "1  Climate change doesnâ€™t cause volcanic eruption...   \n",
       "2  Vaccinated tennis ball boy collapses in the te...   \n",
       "3  North America has experienced an average winte...   \n",
       "4  They're gonna do the same with Climate Change ...   \n",
       "\n",
       "                                        Cleaned_text Sentiment  \\\n",
       "0  solution ive ever heard left propose climate c...       0.0   \n",
       "1      climate change doesnt cause volcanic eruption       0.0   \n",
       "2  vaccinated tennis ball boy collapse tennis cou...      0.25   \n",
       "3  north america experienced average winter tempe...      0.25   \n",
       "4  theyre gonna climate change start get really b...      0.25   \n",
       "\n",
       "   Few Shot Training Example Sentiment_few  Few_Shot_Training_Example  \n",
       "0                      False      negative                      False  \n",
       "1                      False       neutral                      False  \n",
       "2                      False      negative                      False  \n",
       "3                      False       neutral                      False  \n",
       "4                      False      negative                      False  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
