{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##How can an LLM keep track of the current conversation?\n",
        "\n",
        "This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo), which has a maximum of 4,096 input tokens and 4,096 output tokens."
      ],
      "metadata": {
        "id": "wSNBmIoiksRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "0Bo84Sc6lCC-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kup4TYlg7Rnq"
      },
      "outputs": [],
      "source": [
        "# download & install libraries\n",
        "!pip --quiet install openai==0.27.8 # LLM\n",
        "!pip --quiet install langchain==0.0.208 # framework for working with LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.\n",
        "\n",
        "While creating these lessons, I loaded my account with a \\$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \\$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.\n",
        "\n",
        "*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)\n",
        "*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)\n",
        "*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)\n",
        "\n",
        "Example (not a real key):\n",
        "> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk\n",
        "\n"
      ],
      "metadata": {
        "id": "aPD0ViuZlP2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdlxTen6gu5"
      },
      "outputs": [],
      "source": [
        "# paste your OpenAI API key without quotation marks\n",
        "%env OPENAI_API_KEY="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ConversationMemory"
      ],
      "metadata": {
        "id": "3GSRKF_UlS0r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xSgkB0S7eKp"
      },
      "outputs": [],
      "source": [
        "# import LangChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fUQTj37-K-f"
      },
      "outputs": [],
      "source": [
        "# initialize the large language model\n",
        "# will not run without providing an OPENAI_API_KEY above\n",
        "\n",
        "# temperature set to 0.9 for creative output\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLxDLAj8-Kab"
      },
      "outputs": [],
      "source": [
        "# initialize a ConversationChain\n",
        "\n",
        "# verbose = True outputs the conversation history being included with the prompt to the LLM\n",
        "# memory = ConversationBufferMemory stores the current conversation history\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initial prompt\n",
        "conversation.predict(input=\"Pick a random animal and describe it for me.\")\n",
        "\n",
        "\n",
        "# TINKER:\n",
        "\n",
        "# Run several times to verify that the LLM is selecting different animals.\n",
        "#     Notice that on each run the ConversationMemory becomes longer.\n",
        "#     The conversation is being stored and included with each prompt to the LLM.\n",
        "\n",
        "#     If you read through the full history, you'll see that the LLM sometimes\n",
        "#          refers to prior animals it selected."
      ],
      "metadata": {
        "id": "zQbeVVLZBViQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# follow-up prompt\n",
        "conversation.predict(input=\"Would the animal you selected make a good housepet?  Explain why or why not.\")\n",
        "\n",
        "\n",
        "# TINKER:\n",
        "\n",
        "# Write additional follow-up prompts that require the LLM to respond based on\n",
        "#      the conversation history.\n",
        "#      Example:\n",
        "#           conversation.predict(input=\"Which Pokemon do you think would be get along best with the animal you selected?  Explain your response.\")"
      ],
      "metadata": {
        "id": "wDlyFvNkEjMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output full conversation\n",
        "print(conversation)\n",
        "\n",
        "\n",
        "# TINKER:\n",
        "\n",
        "# 1) Disconnect and delete the runtime to reset the ConversationMemory, then\n",
        "#      run with different followup questions.\n",
        "\n",
        "# 2) Remove the memory=ConversationBufferMemory() parameter from the\n",
        "#      ConversationChain, and observe how this impacts the responses.\n",
        "\n",
        "# 3) What do you think will happen if the ConversationMemory being included with\n",
        "#      the prompt is so long that the full prompt exceeds the LLM's input token limit?"
      ],
      "metadata": {
        "id": "H6YEViymBYAk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}