# -*- coding: utf-8 -*-
"""106 VectorDatabase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQTR8migbzirJyGyXNOKdG-PaT49HXJv

##How can new knowledge be added to an LLM?

This project embeds vectors and then retrieves them using an Active Loop database known as Deep Lake.

This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo).

# Setup
"""

# download & install libraries
!pip --quiet install openai # LLM
!pip --quiet install langchain # framework for working with LLMs
!pip --quiet install deeplake[enterprise] # vector database
!pip install -q tiktoken # token tracker
!pip install -q llama_index llama_index_vector_stores_deeplake langchain_community
!pip --quiet install numpy<3.0.0,>=2.0.0 pydantic<3.0.0,>=2.0.0

"""To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.

*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)
*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)

Example (not a real key):
> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk


"""

# paste your OpenAI API key without quotation marks
# %env OPENAI_API_KEY=

"""You'll also need an Active Loop account, which you will use to obtain an Active Loop API token.  Keep your API key secret.

The Active Loop API token is free.

*   [Log into app.ActiveLoop.ai](https://app.activeloop.ai/)
*   Select **+ Add new organization** (top right dropdown menu) â€“- you will use the Organization ID later
*   Click **Create API token** (top right orange button)
*   Set a token name and expiration date
*   On the list of API tokens, click the Copy button next to your token


Example (not a real token):
> %env ACTIVELOOP_TOKEN=kjhlgj234ghafaSDFWRsOItfiuIUTdfljha. qalrj4hglahjsgd234lhgwerlhjg2ejlhgfwdljasfoiasdfiugaDFSDIyiuDFHIUSD98fwhiu2rkjhfeugdfKJGHLDFiFRhfajkr42DFSD.


"""

from google.colab import userdata
# from langchain_community.llms import OpenAIChat # Import OpenAIChat

# load API key from Google Colab Secret
openai_api_key = userdata.get('OPENAI_API_KEY')

"""#Vector Embeddings"""

# import LangChain
from langchain.llms import OpenAI

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import DeepLake
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

# initialize the large language model
# will not run without providing an OPENAI_API_KEY above

# temperature set to 0 for predictable output
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, openai_api_key=openai_api_key)

# load embeddings model
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=openai_api_key)

# set ActiveLoop / Deep Lake path variables
import os
from langchain.vectorstores import DeepLake # import DeepLake from langchain.vectorstores


# paste your Active Loop API token between the ""
os.environ['ACTIVELOOP_TOKEN'] = userdata.get('ACTIVELOOP_TOKEN')
# paste your Active Loop Organization ID between the ""
my_activeloop_org_id = userdata.get('ACTIVELOOP_ORG')

# create a name for your Deep Lake dataset between the ""
#      example: "dataset_01"
# my_activeloop_dataset_name = userdata.get('ACTIVELOOP_TOKEN') # This line is incorrect
my_activeloop_dataset_name = "my_deeplake_dataset" # Replace with a valid dataset name

# create Deep Lake dataset in ActiveLoop account
# will not run without providing an ACTIVELOOP_TOKEN above
dataset_path = f"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}"
db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings) # Use DeepLake from langchain instead of DeepLakeVectorStore

# data to be embedded
texts = [
    "Luke Skywalker was a hobbit who lived in a bat cave beneath a rainbow bridge.",
    "Princess Leia flew B-52 bombers during the Irish Potato Famine",
    "Darth Vader never discovered that Oscar the Grouch was the secret benefactor who paid his tuition at Hogwarts",
    "Jabba the Hutt earned an Oscar for his portrayal of Wolfgang Mozart in The Lego Movie",
]

# split the data to embed as different vectors
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents(texts)

# # explicitly state the import of the DeepLake class from the deeplake package
# !conda install deeplake[enterprise]<4.0.0
# import deeplake

# from langchain.document_loaders import TextLoader
# Add the documents to the vector store
db.add_documents(docs)

# assemble a chain that includes llm, chain_type & retriever
#      chain_type default value = "stuff"
retrieval_qa = RetrievalQA.from_chain_type(
	llm=llm,
	chain_type="stuff",
	retriever=db.as_retriever()
)

# create a list of tools that will be available to an Agent
#      (one tool is included in this list)
tools = [
    Tool(
        name="Retrieval QA System",
        func=retrieval_qa.run,
        description="Useful for answering questions."
    ),
]

# initialize an Agent and provide it with a list of tools
agent = initialize_agent(
	tools,
	llm,
	agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
	verbose=True,
  max_iterations=20
)

# ask a question that the Agent will answer using the vector retrieval tool

# observe how the Agent rephrases the prompt in an attempt to obtain a result
# via cosine similarity
response = agent.run("Where did Luke Skywalker live?")
print(response)


# TINKER:

# 1) In agent.run(), change the prompt to a question that the Agent should
#     be able to answer using vector retrieval on the embedded texts

# 2) When initializing the Agent, change max_iterations and observe how this
#     affects the response

# 3) Comment out the Tool in the list of tools, then observe how this
#     affects the response

# ask a question that the Agent SHOULD be able to answer using the vector
# retrieval tool, but might struggle
response = agent.run("What was Jabba the Hutt known for?")
print(response)


# TINKER:

# Run several times and observe response differences, even with temperature = 0

# same question, this time with more specific directions to guide how the Agent
# responds
response = agent.run("What was Jabba the Hutt known for?  Respond that you don't know if you are not confident in the answer.")
print(response)


# TINKER:

# Run several times and observe the differences in the results, even with temperature = 0

# a question that the Agent will NOT be able to answer using the vector retrieval tool
response = agent.run("Do gophers live at the bottom of the Mariana Trench?")
print(response)

# ask a question that the Agent will think it can answer using the vector
# retrieval tool, but will fail
response = agent.run("What color is Darth Vader's lightsaber?  Respond that you don't know if you are not confident in the answer.")
print(response)


# TINKER:

# In agent.run(), change the prompt to a different question that the Agent
#     will think it can answer using the vector retrieval tool, but will fail

# compare the outputs above to simple llm prompting without vector retrieval
#      (the color of Darth Vader's lightsaber is common knowledge and would have been
#       embedded in training the LLM)
text = "What color is Darth Vader's lightsaber?"
print(llm(text))

text = "What color is Darth Vader's lightsaber? Respond that you don't know if you are not confident in the answer."
print(llm(text))

# new data to be added to the vector database
texts = [
    "Oceanographers were astonished to discover gophers living at the bottom of the Mariana Trench"
]

# split the data to embed as different vectors
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
docs = text_splitter.create_documents(texts)

# embed data as vectors
db.add_documents(docs)

# ask a question that the Agent will answer based on the new vector embedding
response = agent.run("Do gophers live at the bottom of the Mariana Trench?")
print(response)


# TINKER:

# 1) Add additional lines of nonsense to embed in the vector database, then ask
# questions to confirm the Agent is retrieving it.
#      Common knowledge, such as the actual habitat of a gopher, will most likely
#      already be embedded by the LLM during the training process.  Embedding
#      nonsense in the vector database confirms the Agent is responding by
#      retrieving vector embeddings, rather than responding from its the LLM
#      training.

# 2) Ask a question based on information that has not been embedded in the
#      vector database to confirm that the Agent still can respond based
#      solely on the LLM training.