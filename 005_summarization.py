# -*- coding: utf-8 -*-
"""005 Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wcgj03qmvCSB93zMeBfNL9ieujDhpwgI

##How can a text be submitted to an LLM to be summarized?

This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo), which has a maximum of 4,096 input tokens.

#Setup
"""

# download & install libraries
!pip --quiet install openai==0.27.8 # LLM
!pip --quiet install langchain==0.0.208 # framework for working with LLMs

# download & install libraries
!pip --quiet install numpy<3.0.0,>=2.0.0
!pip --quiet install pypdf # load PDFs for LangChain processing
!pip install --quiet -U langchain_community
!pip install -q -U langchain-openai langchain-core
!pip install -q langsmith

"""To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.

*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)
*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)

Example (not a real key):
> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk


"""

# paste your OpenAI API key without quotation marks
# %env OPENAI_API_KEY=

"""#Summarizing a String"""

# import LangChain
from langchain.llms import OpenAI
from langchain import PromptTemplate
# from langchain import OpenAIChat, PromptTemplate
from langchain.chains import LLMChain
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import PyPDFLoader

# initialize the large language model
# will not run without providing an OPENAI_API_KEY above
from google.colab import userdata
# from langchain_community.llms import OpenAIChat # Import OpenAIChat

# load API key from Google Colab Secret
api_key = userdata.get('OPENAI_API_KEY')

# temperature set to 0.9 for creative output
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0.9, openai_api_key=api_key)

# initialize a PromptTemplate with 2 parameters:
#     input_variables – list of value types to use while prompting
#     template – text prompt that includes input_variables

#summarization_template = "Summarize the following text to one sentence: {text}"
#summarization_prompt = PromptTemplate(
#    input_variables = ["text"],
#    template = summarization_template
#    )


summarization_prompt = PromptTemplate(
    input_variables = ["text"],
    template = "Summarize the following text to one sentence: {text}"
    )

# assemble a chain that includes both llm & prompt
summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)

# text to summarize (first 4 paragraphs of Wikipedia's "Large language model" page, 7/23/2024)
sentence1 = "A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification."
sentence2 = " Based on language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process."
sentence3 = " LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word."
text = sentence1 + sentence2 + sentence3

# prompt the LLM
summarized_text = summarization_chain.predict(text=text)
print(summarized_text)


# TINKER:

# 1) Replace sentence1, sentence2 and sentence3 with alternate sentences, then
#     rerun to observe the different output.
#     Example:
#          # text to summarize (first paragraph of Narrative of the Life of Frederick Douglass)
#          text = "I was born in Tuckahoe, near Hillsborough, and about twelve miles from Easton, in Talbot county, Maryland. I have no accurate knowledge of my age, never having seen any authentic record containing it. By far the larger part of the slaves know as little of their ages as horses know of theirs, and it is the wish of most masters within my knowledge to keep their slaves thus ignorant. I do not remember to have ever met a slave who could tell of his birthday. They seldom come nearer to it than planting-time, harvest-time, cherry-time, spring-time, or fall-time. A want of information concerning my own was a source of unhappiness to me even during childhood. The white children could tell their ages. I could not tell why I ought to be deprived of the same privilege. I was not allowed to make any inquiries of my master concerning it. He deemed all such inquiries on the part of a slave improper and impertinent, and evidence of a restless spirit. The nearest estimate I can give makes me now between twenty-seven and twenty-eight years of age. I come to this, from hearing my master say, some time during 1835, I was about seventeen years old."

# 2) Predict what will happen if you submit a text too long for GPT 3.5 Turbo
#      Instruct's input limit of 4,096 tokens.  Submit a very long text and
#      observe the result.

"""Summarizing a PDF using PyPDF
---
"""

# assemble a chain that includes the llm
summarize_chain = load_summarize_chain(llm)

# use PyPDF to load the PDF

# this URL points to "Data Responsibility in Humanitarian Action", a public
# PDF accessiblbe via The Humanitarian Data Exchange at data.humdata.org
document_loader = PyPDFLoader(file_path="https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/77866cc2-d5eb-42c6-995c-75ec9ff02ca8/download/data-responsibility-in-humanitarian-action.pdf")
document = document_loader.load()

# summarize the PDF
summary = summarize_chain(document)
print(summary['output_text'])


# TINKER:

# 1) Load your own PDF files and observe how the LLM summarizes them.
# You can search for publicly available files online.
# An online PDF file can be loaded directly by URL.
# Note that a long PDF might exceed the LLM's token limit.

# 2) Change the LLM temperature and re-run several times to observe how it
#     changes the summary.

from langchain.chat_models import ChatOpenAI
from langchain import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains.summarize import load_summarize_chain
from langchain.document_loaders import WebBaseLoader

def summarize_webpage():
    """Prompts the user for a webpage URL and summarizes its content."""

    # Get the webpage URL from the user
    url = input("Please enter the URL of the webpage you want to summarize: ")

    # Load the webpage content using WebBaseLoader
    loader = WebBaseLoader(url)
    documents = loader.load()

    # Initialize the language model (ensure you have set your OpenAI API key)
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo", api_key=api_key)

    # Create a summarization chain
    chain = load_summarize_chain(llm, chain_type="stuff")

    # Summarize the loaded documents
    summary = chain.run(documents)

    # Print the summary to the console
    print("\nSummary:\n", summary)

# Call the function to start the process
summarize_webpage()

def aspect_based_sentiment_analysis():
    """Prompts the user for a webpage URL and performs aspect-based sentiment analysis."""

    url = input("Please enter the URL of the webpage you want to analyze: ")

    # Load the webpage content using WebBaseLoader
    loader = WebBaseLoader(url)
    documents = loader.load()

    # Initialize the language model (ensure you have set your OpenAI API key)
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo", api_key=api_key)

    # Define the prompt template for aspect-based sentiment analysis
    prompt_template = """
    Given the following text from a webpage:
    {text}

    Please perform aspect-based sentiment analysis. Identify the key aspects
    discussed in the text and determine the sentiment (positive, negative, neutral)
    expressed towards each aspect.

    Present your results in a structured format, for example:

    Aspect: [aspect1]
    Sentiment: [sentiment1]

    Aspect: [aspect2]
    Sentiment: [sentiment2]

    ...
    """

    # Create a prompt template object
    prompt = PromptTemplate(
        input_variables=["text"],
        template=prompt_template,
    )

    # Create an LLM chain with the prompt
    chain = LLMChain(llm=llm, prompt=prompt)

    # Run the chain to perform analysis
    analysis_result = chain.run(documents[0].page_content)  # Assuming one document

    # Print the analysis results
    print("\nAspect-Based Sentiment Analysis:\n", analysis_result)

# Call the function to start the process
aspect_based_sentiment_analysis()

