# -*- coding: utf-8 -*-
"""003 SystemMessage vs. HumanMessage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIIS9PUBA_PMnMm4tepmORwQ0FKxgOjR

##How can you modify the way an LLM approaches a prompt?

**HumanMessage** contains what we normally consider a user prompt – WHAT we want the LLM to consider, such as a specific question or direction.

**SystemMessage** contains the context – HOW we want the LLM to consider it, such as fulfilling a role or taking a specific tone.

This project uses [OpenAI's GPT 3.5 Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo).

#Setup
"""

# download & install libraries
!pip --quiet install openai
!pip install --quiet --upgrade langchain langchain_community openai tiktoken

"""To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.

While creating these lessons, I loaded my account with a \$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.

*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)
*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)
*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)

Example (not a real key):
> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk


"""

from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import ( # Import schema elements directly from langchain
    AIMessage,
    HumanMessage,
    SystemMessage
)
from google.colab import userdata

# load API key from Google Colab Secret
api_key = userdata.get('OPENAI_API_KEY')

# initialize the large language model using ChatOpenAI
# will not run without providing an OPENAI_API_KEY above

# temperature set to 0.9 for creative output
# Initialize ChatOpenAI directly with the api_key
chat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.9, openai_api_key=api_key)
# llm = OpenAI(model_name="gpt-3.5-turbo", temperature=0.9, openai_api_key=api_key)
# chat = chat_models.from_llm(llm) # This line is no longer needed

# temperature set to 0.9 for more creativity
# Pass the api_key directly to the OpenAI class
# Removing proxies argument, passing the api_key while initializing the OpenAI class

"""#System Message vs. Human Message"""

# import LangChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

# SystemMessage - context/instructions the AI model will considet (the HOW)
system_template = "You are an assistant that helps users find information about animals."
system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# HumanMessage – specific prompt the AI will respond to (the WHAT)
human_template = "Provide information about the {animal_factor} of {animal_type}."
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# initialize a chat prompt
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# prompt the LLM
# Get user input for animal type and factor
animal_type = input("Enter the animal type: ")
animal_factor = input("Enter the animal factor (e.g., habitat, diet): ")

# Format the prompt with user input
response = chat(chat_prompt.format_prompt(animal_type=animal_type, animal_factor=animal_factor).to_messages())

# Print the response
print(response.content)


# TINKER:

# Submit an alternate chat prompt to the LLM that uses a different animal and factor.

# Get user input for system template
system_template = input("Enter the system template (AI's persona/instructions): ")
system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

# Get user input for human template
human_template = input("Enter the human template (your question/prompt with placeholders like {animal_type}): ")
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# initialize a chat prompt
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Get user input for animal type and factor (if needed based on the human template)
animal_type = input("Enter the animal type: ")
animal_factor = input("Enter the animal factor (e.g., habitat, diet): ")

# prompt the LLM
response = chat(chat_prompt.format_prompt(animal_type=animal_type, animal_factor=animal_factor).to_messages())
print(response.content)

# TINKER:

# Change the system_template to produce different outputs.
#      Example:
#           system_template = "You are the yippee-skippee, happiest, most cheerful helper on the planet!  You can't help but be overly enthusiastic whenever a user asks a question!  Wow!"