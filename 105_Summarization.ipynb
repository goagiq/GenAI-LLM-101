{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##How can a text be submitted to an LLM to be summarized?\n",
        "\n",
        "This project uses [OpenAI's GPT 3.5 Turbo Instruct](https://platform.openai.com/docs/models/gpt-3-5-turbo), which has a maximum of 4,096 input tokens."
      ],
      "metadata": {
        "id": "eD14r7xnsZbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "VUqYXPKYsmxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kup4TYlg7Rnq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# download & install libraries\n",
        "!pip --quiet install openai==1.10.0 # LLM\n",
        "!pip --quiet install langchain==0.1.4 # framework for working with LLMs\n",
        "!pip --quiet install pypdf==3.10.0 # load PDFs for LangChain processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this project you'll need an OpenAI account, which you will use to obtain an OpenAI API key.  Keep your API key secret.\n",
        "\n",
        "While creating these lessons, I loaded my account with a \\$5.00 credit and turned OFF Auto Recharge.  This means that when the balance reaches \\$0, requests using this API key will stop working.  While creating these lessons, I ended up using only $0.43 in total.\n",
        "\n",
        "*   [Directions for obtaining an OpenAI API Key](https://help.openai.com/en/)\n",
        "*   [Create/manage your OpenAI API keys](https://platform.openai.com/api-keys)\n",
        "*   [Track your OpenAI API key usage/costs](https://platform.openai.com/usage)\n",
        "\n",
        "Example (not a real key):\n",
        "> %env OPENAI_API_KEY=uy-test-sdf87ewrkhjcdsoiewe2DSFIIF234234jhk23rHJJKH323jk\n",
        "\n"
      ],
      "metadata": {
        "id": "enl4iQyksuDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPdlxTen6gu5"
      },
      "outputs": [],
      "source": [
        "# paste your OpenAI API key without quotation marks\n",
        "%env OPENAI_API_KEY="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarizing a String"
      ],
      "metadata": {
        "id": "PqSE-rrpwfdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xSgkB0S7eKp"
      },
      "outputs": [],
      "source": [
        "# import LangChain\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fUQTj37-K-f"
      },
      "outputs": [],
      "source": [
        "# initialize the large language model\n",
        "# will not run without providing an OPENAI_API_KEY above\n",
        "\n",
        "# temperature set to 0 for predictable output\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a PromptTemplate with 2 parameters:\n",
        "#     input_variables – list of value types to use while prompting\n",
        "#     template – text prompt that includes input_variables\n",
        "\n",
        "#summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
        "#summarization_prompt = PromptTemplate(\n",
        "#    input_variables = [\"text\"],\n",
        "#    template = summarization_template\n",
        "#    )\n",
        "\n",
        "\n",
        "summarization_prompt = PromptTemplate(\n",
        "    input_variables = [\"text\"],\n",
        "    template = \"Summarize the following text to one sentence: {text}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "1EZZb_TFxWWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble a chain that includes both llm & prompt\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
      ],
      "metadata": {
        "id": "2xZOowtvzW_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text to summarize (first 4 paragraphs of Wikipedia's \"Large language model\" page, 7/23/2024)\n",
        "sentence1 = \"A large language model (LLM) is a computational model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification.\"\n",
        "sentence2 = \" Based on language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive self-supervised and semi-supervised training process.\"\n",
        "sentence3 = \" LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\"\n",
        "text = sentence1 + sentence2 + sentence3"
      ],
      "metadata": {
        "id": "TxhC0B1DhG9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt the LLM\n",
        "summarized_text = summarization_chain.predict(text=text)\n",
        "print(summarized_text)\n",
        "\n",
        "\n",
        "# TINKER:\n",
        "\n",
        "# 1) Replace sentence1, sentence2 and sentence3 with alternate sentences, then\n",
        "#     rerun to observe the different output.\n",
        "#     Example:\n",
        "#          # text to summarize (first paragraph of Narrative of the Life of Frederick Douglass)\n",
        "#          text = \"I was born in Tuckahoe, near Hillsborough, and about twelve miles from Easton, in Talbot county, Maryland. I have no accurate knowledge of my age, never having seen any authentic record containing it. By far the larger part of the slaves know as little of their ages as horses know of theirs, and it is the wish of most masters within my knowledge to keep their slaves thus ignorant. I do not remember to have ever met a slave who could tell of his birthday. They seldom come nearer to it than planting-time, harvest-time, cherry-time, spring-time, or fall-time. A want of information concerning my own was a source of unhappiness to me even during childhood. The white children could tell their ages. I could not tell why I ought to be deprived of the same privilege. I was not allowed to make any inquiries of my master concerning it. He deemed all such inquiries on the part of a slave improper and impertinent, and evidence of a restless spirit. The nearest estimate I can give makes me now between twenty-seven and twenty-eight years of age. I come to this, from hearing my master say, some time during 1835, I was about seventeen years old.\"\n",
        "\n",
        "# 2) Predict what will happen if you submit a text too long for GPT 3.5 Turbo\n",
        "#      Instruct's input limit of 4,096 tokens.  Submit a very long text and\n",
        "#      observe the result."
      ],
      "metadata": {
        "id": "2jJLSjMghnu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarizing a PDF using PyPDF\n",
        "---"
      ],
      "metadata": {
        "id": "H4uQu-zDttVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble a chain that includes the llm\n",
        "summarize_chain = load_summarize_chain(llm)"
      ],
      "metadata": {
        "id": "eExMLZBCsD9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use PyPDF to load the PDF\n",
        "\n",
        "# this URL points to \"Data Responsibility in Humanitarian Action\", a public\n",
        "# PDF accessiblbe via The Humanitarian Data Exchange at data.humdata.org\n",
        "document_loader = PyPDFLoader(file_path=\"https://data.humdata.org/dataset/2048a947-5714-4220-905b-e662cbcd14c8/resource/77866cc2-d5eb-42c6-995c-75ec9ff02ca8/download/data-responsibility-in-humanitarian-action.pdf\")\n",
        "document = document_loader.load()"
      ],
      "metadata": {
        "id": "atNBS8V3sLrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the PDF\n",
        "summary = summarize_chain(document)\n",
        "print(summary['output_text'])\n",
        "\n",
        "\n",
        "# TINKER:\n",
        "\n",
        "# 1) Load your own PDF files and observe how the LLM summarizes them.\n",
        "# You can search for publicly available files online.\n",
        "# An online PDF file can be loaded directly by URL.\n",
        "# Note that a long PDF might exceed the LLM's token limit.\n",
        "\n",
        "# 2) Change the LLM temperature and re-run several times to observe how it\n",
        "#     changes the summary."
      ],
      "metadata": {
        "id": "7BTwJfYAtDKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}